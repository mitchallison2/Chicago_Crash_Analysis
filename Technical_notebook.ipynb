{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer,  make_column_selector as selector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix, recall_score,\\\n",
    "    accuracy_score, precision_score, f1_score\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImPipeline\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This project uses crash data from Chicago in order to create a model that can predict what type of crash has occured. The model uses data from the crash, person, and vehicle databases.\n",
    "\n",
    "Chicago can use this model to determine what kind of crash has occured by only inputting 23 simple answers.\n",
    "\n",
    "This project used an iterative approach to answering the business problem, so this project will be explained chronologically to best illustrate how the problem was approached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "The cause of a crash, known as the `crash type`, can be difficult to surmise. There are lots of variables at play and people can lie or be un/nonresponsive when questioned in more serious cases.\n",
    "\n",
    "Our goal is to create a model that, given 23 simple inputs, can predict the cause of a car accident in Chicago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intake Data\n",
    "\n",
    "The intake datasets include records from 2015 to August 2022.\n",
    "\n",
    "`Traffic_Crashes_-_Vehicles.csv` - Information about vehicle status at time of crash. Includes variables such as vehicle defect, number of crashes, and vehicle length.\n",
    "\n",
    "`Traffic_Crashes_-_People.csv` - Information about person status at time of crash. Includes variables such as drivers licence type, origin, sex, and age.\n",
    "\n",
    "`Traffic_Crashes_-_Crashes.csv` - Information about the crash's environmental factors. Includes variables such as weather, street defect, and beat of occurance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation(Round 1)\n",
    "\n",
    "The first step taken is to examine the `crashes` data using the profile tool. 34 steps were identified to set up the dataframe for modeling. Some notable steps taken were to remove columns that weren't helpful or had a lot of missing values.\n",
    "\n",
    "At this point several variables were engineered as well to help simplify the modeling. One big variable that was engineered was to condense all of the `injury` fields into an `INJURY_LEVEL` to reduce complexity. This level increases as the level of injury increases.(no injury/non-incapacitating/incapacitating/fatal)\n",
    "\n",
    "The `crashes` dataframe was reduced from 49 to 18 columns.\n",
    "Although there are some difficult values here still, we can still use this data to create our first model.\n",
    "\n",
    "It is also becomind evident that further cleaning may be required. Most of these variables are categorical, with many(10+) categories. This dataset alone is also fairly large at 643k datapoints.\n",
    "\n",
    "(FROM ./EDA_cleaning/EDA_CRASHES.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes = pd.read_csv('./Datasets/Traffic_Crashes_-_Crashes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRASH_RECORD_ID', 'RD_NO', 'CRASH_DATE_EST_I', 'CRASH_DATE',\n",
       "       'POSTED_SPEED_LIMIT', 'TRAFFIC_CONTROL_DEVICE', 'DEVICE_CONDITION',\n",
       "       'WEATHER_CONDITION', 'LIGHTING_CONDITION', 'FIRST_CRASH_TYPE',\n",
       "       'TRAFFICWAY_TYPE', 'LANE_CNT', 'ALIGNMENT', 'ROADWAY_SURFACE_COND',\n",
       "       'ROAD_DEFECT', 'REPORT_TYPE', 'CRASH_TYPE', 'INTERSECTION_RELATED_I',\n",
       "       'NOT_RIGHT_OF_WAY_I', 'HIT_AND_RUN_I', 'DAMAGE', 'DATE_POLICE_NOTIFIED',\n",
       "       'PRIM_CONTRIBUTORY_CAUSE', 'SEC_CONTRIBUTORY_CAUSE', 'STREET_NO',\n",
       "       'STREET_DIRECTION', 'STREET_NAME', 'BEAT_OF_OCCURRENCE',\n",
       "       'PHOTOS_TAKEN_I', 'STATEMENTS_TAKEN_I', 'DOORING_I', 'WORK_ZONE_I',\n",
       "       'WORK_ZONE_TYPE', 'WORKERS_PRESENT_I', 'NUM_UNITS',\n",
       "       'MOST_SEVERE_INJURY', 'INJURIES_TOTAL', 'INJURIES_FATAL',\n",
       "       'INJURIES_INCAPACITATING', 'INJURIES_NON_INCAPACITATING',\n",
       "       'INJURIES_REPORTED_NOT_EVIDENT', 'INJURIES_NO_INDICATION',\n",
       "       'INJURIES_UNKNOWN', 'CRASH_HOUR', 'CRASH_DAY_OF_WEEK', 'CRASH_MONTH',\n",
       "       'LATITUDE', 'LONGITUDE', 'LOCATION'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crashes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickleFile = open('./Datasets/clean_crashes.pkl', 'rb')\n",
    "crashes_clean = pickle.load(unpickleFile, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRASH_RECORD_ID', 'POSTED_SPEED_LIMIT', 'WEATHER_CONDITION',\n",
       "       'LIGHTING_CONDITION', 'FIRST_CRASH_TYPE', 'TRAFFICWAY_TYPE',\n",
       "       'ALIGNMENT', 'ROADWAY_SURFACE_COND', 'ROAD_DEFECT', 'CRASH_TYPE',\n",
       "       'PRIM_CONTRIBUTORY_CAUSE', 'SEC_CONTRIBUTORY_CAUSE', 'STREET_DIRECTION',\n",
       "       'BEAT_OF_OCCURRENCE', 'CRASH_HOUR', 'CRASH_DAY_OF_WEEK', 'CRASH_MONTH',\n",
       "       'INJURY_LEVEL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crashes_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## First Simple Model\n",
    "\n",
    "With several components of this clean crashes dataframe, we went ahead and created our first simple model.\n",
    "\n",
    "This first simple model is only trained on `WEATHER_CONDITION` and `POSTED_SPEED_LIMIT` to predict the target of `PRIM_CONTRIBUTORY_CAUSE`. We used these variables arbitrarily, but looking back we could have first used linear regression to find the most important variables.\n",
    "\n",
    "We also made sure to exclude variables that described events that had no effect on the crash. These mostly include internal report metrics by Chicago.\n",
    "\n",
    "For our first model we made sure to use a train test split and employ pipelines to reduce data leakage, as well as to practice the process and see if we could solve any problems that may happen to later models now. We did run into some problems using SimpleImputer with this data, but would circle back to this later.\n",
    "\n",
    "The model used was a MultiOutputClassifier using LogisticRegression, and had a score of only 0.38. At this point we know that we need to add additional factors, and reduce the prediction target classes to increase model performance.\n",
    "\n",
    "(FROM ./EDA_cleaning/EDA_CRASHES.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickleFile = open('./EDA_cleaning/FSM.sav', 'rb')\n",
    "FSM = pickle.load(unpickleFile, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickleFile = open('./EDA_cleaning/FSM_Xtest_ct.sav', 'rb')\n",
    "FSM_X_test_ct = pickle.load(unpickleFile, encoding='bytes')\n",
    "\n",
    "unpickleFile = open('./EDA_cleaning/FSM_ytest.sav', 'rb')\n",
    "FSM_y_test = pickle.load(unpickleFile, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37907858741606565"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FSM.score(FSM_X_test_ct, FSM_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation(Round 2)\n",
    "\n",
    "## Combine `crash` with `person` and `vehicle`\n",
    "\n",
    "Next, we'll go ahead and examine the `person` and `vehicle` datasets.\n",
    "\n",
    "`person` - Again work on reducing complexity. No variables were engineered at this point, but instead we got rid of many columns that weren't helpful. Reduced variables from 30 to 7. Dropped rows that included passenger data, as we're assuming the driver was controlling the vehicle when it crashed.\n",
    "\n",
    "`vehicle` - Reduced complexity. Reduced variables from 72 to 11.\n",
    "\n",
    "Then these dataframes were combined with the `crash` dataset, such that every row became:\n",
    "A single vehicle operator, with the information about their vehicle and the crash they were in.\n",
    "\n",
    "With this dataframe, we could create models that look at each driver's information and use that to predict the type of crash that driver got into.\n",
    "We ended up with 29 variables and 1 categorical target, and 1.1 million datapoints.\n",
    "\n",
    "- (FROM ./EDA_cleaning/EDA_People.ipynb)\n",
    "- (FROM ./EDA_cleaning/EDA_Vehicles.ipynb)\n",
    "- (FROM ./EDA_cleaning/joined_dataset(person-vehicle-crash).ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickleFile = open('./Datasets/clean_joined_df.pkl', 'rb')\n",
    "combined_df =  pickle.load(unpickleFile, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DRIVERS_LICENSE_CLASS', 'SEX', 'AGE', 'SAFETY_EQUIPMENT', 'UNIT_TYPE',\n",
       "       'NUM_PASSENGERS', 'MAKE', 'MODEL', 'VEHICLE_DEFECT', 'VEHICLE_TYPE',\n",
       "       'VEHICLE_USE', 'MANEUVER', 'FIRST_CONTACT_POINT', 'POSTED_SPEED_LIMIT',\n",
       "       'WEATHER_CONDITION', 'LIGHTING_CONDITION', 'FIRST_CRASH_TYPE',\n",
       "       'TRAFFICWAY_TYPE', 'ALIGNMENT', 'ROADWAY_SURFACE_COND', 'ROAD_DEFECT',\n",
       "       'CRASH_TYPE', 'PRIM_CONTRIBUTORY_CAUSE', 'SEC_CONTRIBUTORY_CAUSE',\n",
       "       'STREET_DIRECTION', 'BEAT_OF_OCCURRENCE', 'CRASH_HOUR',\n",
       "       'CRASH_DAY_OF_WEEK', 'CRASH_MONTH', 'INJURY_LEVEL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1088146, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DRIVERS_LICENSE_CLASS       object\n",
       "SEX                         object\n",
       "AGE                        float64\n",
       "SAFETY_EQUIPMENT            object\n",
       "UNIT_TYPE                   object\n",
       "NUM_PASSENGERS             float64\n",
       "MAKE                        object\n",
       "MODEL                       object\n",
       "VEHICLE_DEFECT              object\n",
       "VEHICLE_TYPE                object\n",
       "VEHICLE_USE                 object\n",
       "MANEUVER                    object\n",
       "FIRST_CONTACT_POINT         object\n",
       "POSTED_SPEED_LIMIT         float64\n",
       "WEATHER_CONDITION           object\n",
       "LIGHTING_CONDITION          object\n",
       "FIRST_CRASH_TYPE            object\n",
       "TRAFFICWAY_TYPE             object\n",
       "ALIGNMENT                   object\n",
       "ROADWAY_SURFACE_COND        object\n",
       "ROAD_DEFECT                 object\n",
       "CRASH_TYPE                  object\n",
       "PRIM_CONTRIBUTORY_CAUSE     object\n",
       "SEC_CONTRIBUTORY_CAUSE      object\n",
       "STREET_DIRECTION            object\n",
       "BEAT_OF_OCCURRENCE         float64\n",
       "CRASH_HOUR                 float64\n",
       "CRASH_DAY_OF_WEEK          float64\n",
       "CRASH_MONTH                float64\n",
       "INJURY_LEVEL                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch (Round 1)\n",
    "\n",
    "After practicing pipelines with the FSM, I felt comfortable working on running gridsearches to find a better performing model was the next step. Pipelines with proper steps taken to avoid data leakage was taken.\n",
    "\n",
    "In the first round of gridsearches, I found that after combining the cleaned versions of each dataset, gridsearching was just not working.\n",
    "\n",
    "It took 3 days to troubleshoot the causes of the issues, likely due to my own inexperience. This could also be because I did not double-check the tools that I was using to make sure that they were all appropriate.\n",
    "\n",
    "These issues are detailed below:\n",
    "\n",
    "- SMOTE USAGE: \n",
    "Our targets were very imbalanced, as you can see on the graph below. The top class was more than the 2nd and 3rd class added together. I wanted to use SMOTE to help fix this issue to create a better model. This is due to the size of the dataset(1.3 million). Trying to use SMOTE on such a large dataset, with so many variables after applying OHE, made this too computationally expensive. FIX: Use undersampling or a model that compensates for class imbalance.\n",
    "![target imbalance](./graphs/all_causes.jpg)\n",
    "\n",
    "\n",
    "- TOO MANY TARGETS:\n",
    "As mentioned above, there was some fairly serious class imbalance. The top cause was 'unable to determine' which also isn't helpful for predicting. How is it helpful to feed the model information and the model tells you that it's 'unable to determine' the cause? FIX: Reducing the targets. This would also decrease the size of the dataset which would increase the speed of gridsearches.\n",
    "\n",
    "\n",
    "- TOO MANY VARIABLES/TOO COMPLEX:\n",
    "We were able to reduce the original data to only 29 variables. However, all of these variables are categorical, which makes the OHE applied dataframe absolutely gigantic. FIX: additional data cleaning/engineering would need to be applied.\n",
    "\n",
    "\n",
    "- SIMPLEIMPUTER NOT OPERATING CORRECTLY:\n",
    "I'm still unsure of the cause of this issue, but using the imputer as a part of the pipelines on my gridsearches caused them to become unfunctional.\n",
    "\n",
    "(FROM ./Modeling_gridsearch/gridsearch1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation(Round 3)\n",
    "\n",
    "We'll need to apply the fixes for the issues found in the first round of gridsearching. At this point, I was also running critically low on time in order to have the deliverables ready on time. We would need to take a creative approach to approaching this issue.\n",
    "\n",
    "Although this may have introduced some data leakage or muddied the data, I made the decision to create engineered variables for most categorical values.\n",
    "\n",
    "So for example with `weather`, instead of inputting 'clear', 'other', 'snow', 'sleet', etc, you can input only 3 options for the weather.\n",
    "\n",
    "This has a couple of benefits:\n",
    "- greatly reduced complexity\n",
    "- removes need for additional cleaning with pipelines(OHE only)\n",
    "\n",
    "I also only screened the data to predict upon the top 4 causes, except the #1 cause of 'unable to determine'.\n",
    "![target selection](./graphs/top_4_causes.jpg)\n",
    "\n",
    "We also took addtitional steps to screen the data and eliminate excessively complicated variables, and I've detailed the cleaning steps taken below, as well as included the functions created to do a final clean pass on the data.\n",
    "\n",
    "We were able to again reduce the complexity of the dataframe, going down to 21 variables(11 simple variables) and only 4 target classes. We've been able to get down to only 230k data points, which will greatly speed up gridsearches.\n",
    "\n",
    "(FROM ./EDA_cleaning/final_data_prep.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of variables to screen with\n",
    "# WILL TRY ON TARGET FIRST AND THEN SEE HOW MUCH SCREENING IS NECESSARY\n",
    "\n",
    "# 'PRIM_CONTRIBUTORY_CAUSE' - limit to top 4:\n",
    "# FAILING TO YIELD RIGHT-OF-WAY                                                       133013\n",
    "# FOLLOWING TOO CLOSELY                                                               132795\n",
    "# IMPROPER OVERTAKING/PASSING                                                          57294\n",
    "# FAILING TO REDUCE SPEED TO AVOID CRASH                                               51012\n",
    "\n",
    "\n",
    "# 'DRIVERS_LICENSE_CLASS - only take people with drivers licenses (D)\n",
    "# 'UNIT_TYPE' - only for drivers\n",
    "# 'AGE' - only people 16 or above can use a drivers license\n",
    "\n",
    "# list of variables to engineer\n",
    "# SAFETY_EQUIPMENT - safety belt used/not used\n",
    "# 'NUM_PASSENGERS' - has/does not have passengers\n",
    "# 'VEHICLE_DEFECT' - defective/not defective\n",
    "# 'VEHICLE_TYPE' - motorcycle/passenger/large passenger/large\n",
    "# 'VEHICLE_USE' - personal/not-personal\n",
    "# 'MANEUVER' - straight/turn/traffic/other\n",
    "# 'FIRST_CONTACT_POINT' - LEAVE AS IS FOR NOW\n",
    "# 'POSTED_SPEED_LIMIT' - BUCKET(LOW/MED/HIGH)\n",
    "# 'WEATHER_CONDITION' - clear+unknown/rain/snow\n",
    "# 'ROADWAY_SURFACE_COND' - DRY/WET/OTHER\n",
    "# 'ROAD_DEFECT' - no defect/possible defect\n",
    "# 'TRAFFICWAY_TYPE'' - not divided/divided/other\n",
    "# 'ALIGNMENT' - straight/curved\n",
    "\n",
    "# Overly complicated: remove with predjudice\n",
    "# 'MAKE', 'MODEL', 'FIRST_CRASH_TYPE', 'SEC_CONTRIBUTORY_CAUSE', 'BEAT_OF_OCCURRENCE'\n",
    "\n",
    "# list of variables to remove after above steps\n",
    "# 'DRIVERS_LICENSE_CLASS', 'SAFETY_EQUIPMENT', 'UNIT_TYPE', 'NUM_PASSENGERS', 'VEHICLE_DEFECT', 'VEHICLE_TYPE',\n",
    "# 'VEHICLE_USE', 'MANEUVER', 'POSTED_SPEED_LIMIT', 'WEATHER_CONDITION', 'ROADWAY_SURFACE_COND', 'ALIGNMENT',\n",
    "# 'ROAD_DEFECT'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def screen_vars(data):\n",
    "    \n",
    "    # SCREEN pass 1: target reduction\n",
    "    cause_list = ['FAILING TO YIELD RIGHT-OF-WAY', 'FOLLOWING TOO CLOSELY',\n",
    "              'IMPROPER OVERTAKING/PASSING', 'FAILING TO REDUCE SPEED TO AVOID CRASH']\n",
    "    data = data[data['PRIM_CONTRIBUTORY_CAUSE'].isin(cause_list)]\n",
    "    \n",
    "    # SCREEN pass 2: has standard drivers license\n",
    "#     not looking at crashes from drivers of industrial applications\n",
    "    data = data[data['DRIVERS_LICENSE_CLASS'].isin(['D'])]\n",
    "#     data.DRIVERS_LICENSE_CLASS.value_counts()\n",
    "\n",
    "    # SCREEN pass 3: UNIT_TYPE only for drivers\n",
    "    data = data[data['UNIT_TYPE'].isin(['DRIVER'])]\n",
    "    \n",
    "    # SCREEN pass 4: age 16 or above\n",
    "#     Can't have a drivers licence below 16\n",
    "    data = data[data['AGE'] >= 16]\n",
    "    \n",
    "    # DELETE pass: remove overly complex variables\n",
    "    data.drop(columns=['MAKE', 'MODEL', 'FIRST_CRASH_TYPE', 'SEC_CONTRIBUTORY_CAUSE', 'BEAT_OF_OCCURRENCE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENGINEER pass: create function to engineer new variables for all required cols\n",
    "\n",
    "def Engineer_Vars(data):\n",
    "    \n",
    "    # 'SAFETY_EQUIPMENT' - safety belt or helmet used/not used+unkown\n",
    "    data['se_simple'] = np.where(((data['SAFETY_EQUIPMENT']=='SAFETY BELT USED')|\n",
    "                               (data['SAFETY_EQUIPMENT']=='DOT COMPLIANT MOTORCYCLE HELMET')), 'safety belt/helmet used', \n",
    "                                 'NO safety belt/helmet + unknown')\n",
    "    \n",
    "    # 'NUM_PASSENGERS' - has/does not have passengers\n",
    "    data['passengers_simple'] = np.where(data['NUM_PASSENGERS'] > 0, 'has passengers', 'no passengers')\n",
    "    \n",
    "    # 'VEHICLE_DEFECT' - not defective/defective+unknown\n",
    "    data['defect_simple'] = np.where(data['VEHICLE_DEFECT'] == 'NONE', 'not defective', 'defective/unknown')\n",
    "    \n",
    "    # 'VEHICLE_TYPE' - passenger/other\n",
    "    data['vehicletype_simple'] = np.where(data['VEHICLE_TYPE'] == 'PASSENGER', 'passenger car', 'other')\n",
    "    \n",
    "    # 'VEHICLE_USE' - personal/not-personal\n",
    "    data['vehicleuse_simple'] = np.where(data['VEHICLE_USE'] == 'PERSONAL', 'personal vehicle', 'non-personal vehicle')  \n",
    "    \n",
    "    # 'MANEUVER' - straight/turn/traffic/other    \n",
    "#     Leave as is for now. too complicated\n",
    "\n",
    "    # 'FIRST_CONTACT_POINT' - LEAVE AS IS FOR NOW\n",
    "    \n",
    "    # 'POSTED_SPEED_LIMIT' - BUCKET(LOW/MED/HIGH)\n",
    "    conditions = [\n",
    "        (data['POSTED_SPEED_LIMIT'] < 30),\n",
    "        (data['POSTED_SPEED_LIMIT'] >= 30) & (data['POSTED_SPEED_LIMIT'] < 40),\n",
    "        (data['POSTED_SPEED_LIMIT'] >= 40)]\n",
    "    \n",
    "    choices = ['low', 'med', 'high']\n",
    "    data['speedlimit_simple'] = np.select(conditions, choices, default='low')\n",
    "    \n",
    "    # 'WEATHER_CONDITION' - clear+unknown/rain/snow/other\n",
    "    conditions = [\n",
    "        (data['WEATHER_CONDITION'] == 'CLEAR') | (data['WEATHER_CONDITION'] == 'UNKNOWN'),\n",
    "        (data['WEATHER_CONDITION'] == 'RAIN') | (data['WEATHER_CONDITION'] == 'CLOUDY/OVERCAST') |\n",
    "            (data['WEATHER_CONDITION'] == 'CLOUDY/OVERCAST'),\n",
    "        (data['WEATHER_CONDITION'] == 'SNOW')]\n",
    "    \n",
    "    choices = ['clear/unknown', 'rain', 'snow']\n",
    "    data['weather_simple'] = np.select(conditions, choices, default='other')\n",
    "    \n",
    "    # 'ROADWAY_SURFACE_COND' - DRY/H20/OTHER\n",
    "    conditions = [\n",
    "        (data['ROADWAY_SURFACE_COND'] == 'DRY'),\n",
    "        (data['ROADWAY_SURFACE_COND'] == 'WET') | (data['ROADWAY_SURFACE_COND'] == 'SNOW OR SLUSH') |\n",
    "            (data['ROADWAY_SURFACE_COND'] == 'ICE')]\n",
    "    \n",
    "    choices = ['dry', 'H20']\n",
    "    data['roadcond_simple'] = np.select(conditions, choices, default='other')\n",
    "    \n",
    "    # 'ROAD_DEFECT' - no defect/possible defect\n",
    "    data['roaddef_simple'] = np.where(data['ROAD_DEFECT'] == 'NO DEFECTS', 'no road defect', 'possible road defect')\n",
    "    \n",
    "    # 'TRAFFICWAY_TYPE'' - not divided/divided/other\n",
    "    conditions = [\n",
    "        (data['TRAFFICWAY_TYPE'] == 'NOT DIVIDED') | (data['TRAFFICWAY_TYPE'] == 'ONE-WAY'),\n",
    "        (data['TRAFFICWAY_TYPE'] == 'DIVIDED - W/MEDIAN (NOT RAISED)') | (data['TRAFFICWAY_TYPE'] == 'DIVIDED - W/MEDIAN BARRIER')\n",
    "        ]\n",
    "    \n",
    "    choices = ['not divided', 'divided']\n",
    "    data['trafficway_simple'] = np.select(conditions, choices, default='other')\n",
    "    \n",
    "    # 'ALIGNMENT' - straight/curved\n",
    "    data['alignment_simple'] = np.where(data['ALIGNMENT'].str.contains('STRAIGHT'), 'straight', 'curved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SEX', 'AGE', 'MANEUVER', 'FIRST_CONTACT_POINT', 'LIGHTING_CONDITION',\n",
       "       'CRASH_TYPE', 'PRIM_CONTRIBUTORY_CAUSE', 'STREET_DIRECTION',\n",
       "       'CRASH_HOUR', 'CRASH_DAY_OF_WEEK', 'CRASH_MONTH', 'INJURY_LEVEL',\n",
       "       'se_simple', 'passengers_simple', 'defect_simple', 'vehicletype_simple',\n",
       "       'vehicleuse_simple', 'speedlimit_simple', 'weather_simple',\n",
       "       'roadcond_simple', 'roaddef_simple', 'trafficway_simple',\n",
       "       'alignment_simple'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpickleFile = open('./Datasets/final_data.pkl', 'rb')\n",
    "final_df =  pickle.load(unpickleFile, encoding='bytes')\n",
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230514, 23)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch (Round 2)\n",
    "\n",
    "Now that we have a dataframe that's of a manageable size, with an appropriate amount of targets, we can continue with some real gridsearches!\n",
    "\n",
    "For our gridsearches, we had to apply several transformations:\n",
    "- For categorical data: apply onehotencoder so that models can be made from categorical data\n",
    "- Scaled data so that it could better be modeled\n",
    "- Encode target data so that it could be modeled\n",
    "\n",
    "I ran gridsearches with RandomForestClassifier, DecisionTreeClassifier, and GradientBostingClassifier.\n",
    "Although I didn't run as many gridsearches as I would have liked, the best model ended up being a RandomForestClassifier with these criterion:\n",
    "\n",
    "{'rfc__criterion': 'gini',\n",
    " 'rfc__max_features': 'sqrt',\n",
    " 'rfc__min_samples_leaf': 2}\n",
    " \n",
    " This model was able to achieve a score of 0.64 on the test data.\n",
    " \n",
    " (FROM ./Modeling_gridsearch/gridsearch2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickleFile = open('./Modeling_gridsearch/rfc_model_final.sav', 'rb')\n",
    "final_model =  pickle.load(unpickleFile, encoding='bytes')\n",
    "\n",
    "\n",
    "\n",
    "unpickleFile = open('./Modeling_gridsearch/rfc_model_final_Xtest.sav', 'rb')\n",
    "final_model_Xtest =  pickle.load(unpickleFile, encoding='bytes')\n",
    "\n",
    "unpickleFile = open('./Modeling_gridsearch/rfc_model_final_ytest.sav', 'rb')\n",
    "final_model_ytest =  pickle.load(unpickleFile, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6370924360998803"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.score(final_model_Xtest, final_model_ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Analysis\n",
    "\n",
    "The final model has the following performance statistics:\n",
    "- Recall: 0.64\n",
    "- Accuracy: 0.64\n",
    "- Precision: 0.64\n",
    "- F1: 0.64\n",
    "\n",
    "![confusion matrix](./graphs/confusion_matrix.jpg)\n",
    "These stats are all the same because we used the 'micro' average hyperparameter for the scoring. This adds up the values in the confusion matrix before applying division. This is the recommended hyperparameter for unbalanced data.\n",
    "\n",
    "Using 'macro' would have applied division and then summed the values together, and so the performance statistics would have been much lower.\n",
    "\n",
    "We also wanted to look at what the most important factors are when determining the type of crash. Unsuprisingly, the most imporant factors are sex and age, which are the first two columns fed into the model.\n",
    "\n",
    "(FROM ./Modeling_gridsearch/gridsearch2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.07904\n",
      "Feature: 1, Score: 0.06944\n",
      "Feature: 2, Score: 0.05088\n",
      "Feature: 3, Score: 0.06060\n",
      "Feature: 4, Score: 0.01246\n",
      "Feature: 5, Score: 0.01247\n",
      "Feature: 6, Score: 0.00004\n",
      "Feature: 7, Score: 0.00094\n",
      "Feature: 8, Score: 0.00594\n",
      "Feature: 9, Score: 0.00522\n",
      "Feature: 10, Score: 0.00000\n",
      "Feature: 11, Score: 0.00002\n",
      "Feature: 12, Score: 0.00386\n",
      "Feature: 13, Score: 0.00958\n",
      "Feature: 14, Score: 0.00071\n",
      "Feature: 15, Score: 0.00163\n",
      "Feature: 16, Score: 0.00014\n",
      "Feature: 17, Score: 0.00143\n",
      "Feature: 18, Score: 0.03821\n",
      "Feature: 19, Score: 0.00180\n",
      "Feature: 20, Score: 0.00065\n",
      "Feature: 21, Score: 0.00021\n",
      "Feature: 22, Score: 0.00044\n",
      "Feature: 23, Score: 0.04480\n",
      "Feature: 24, Score: 0.00141\n",
      "Feature: 25, Score: 0.01870\n",
      "Feature: 26, Score: 0.02782\n",
      "Feature: 27, Score: 0.00001\n",
      "Feature: 28, Score: 0.00386\n",
      "Feature: 29, Score: 0.00064\n",
      "Feature: 30, Score: 0.00148\n",
      "Feature: 31, Score: 0.01474\n",
      "Feature: 32, Score: 0.01704\n",
      "Feature: 33, Score: 0.01641\n",
      "Feature: 34, Score: 0.00169\n",
      "Feature: 35, Score: 0.01593\n",
      "Feature: 36, Score: 0.06613\n",
      "Feature: 37, Score: 0.05010\n",
      "Feature: 38, Score: 0.00474\n",
      "Feature: 39, Score: 0.00882\n",
      "Feature: 40, Score: 0.00792\n",
      "Feature: 41, Score: 0.02295\n",
      "Feature: 42, Score: 0.01275\n",
      "Feature: 43, Score: 0.00263\n",
      "Feature: 44, Score: 0.00169\n",
      "Feature: 45, Score: 0.00337\n",
      "Feature: 46, Score: 0.00806\n",
      "Feature: 47, Score: 0.00150\n",
      "Feature: 48, Score: 0.00943\n",
      "Feature: 49, Score: 0.00281\n",
      "Feature: 50, Score: 0.00085\n",
      "Feature: 51, Score: 0.01568\n",
      "Feature: 52, Score: 0.01555\n",
      "Feature: 53, Score: 0.00561\n",
      "Feature: 54, Score: 0.01167\n",
      "Feature: 55, Score: 0.01336\n",
      "Feature: 56, Score: 0.01319\n",
      "Feature: 57, Score: 0.00496\n",
      "Feature: 58, Score: 0.00423\n",
      "Feature: 59, Score: 0.00138\n",
      "Feature: 60, Score: 0.00004\n",
      "Feature: 61, Score: 0.00950\n",
      "Feature: 62, Score: 0.00939\n",
      "Feature: 63, Score: 0.00842\n",
      "Feature: 64, Score: 0.00837\n",
      "Feature: 65, Score: 0.00817\n",
      "Feature: 66, Score: 0.00830\n",
      "Feature: 67, Score: 0.01065\n",
      "Feature: 68, Score: 0.01061\n",
      "Feature: 69, Score: 0.00632\n",
      "Feature: 70, Score: 0.00634\n",
      "Feature: 71, Score: 0.00484\n",
      "Feature: 72, Score: 0.00598\n",
      "Feature: 73, Score: 0.00590\n",
      "Feature: 74, Score: 0.00701\n",
      "Feature: 75, Score: 0.00066\n",
      "Feature: 76, Score: 0.00644\n",
      "Feature: 77, Score: 0.00249\n",
      "Feature: 78, Score: 0.00750\n",
      "Feature: 79, Score: 0.00843\n",
      "Feature: 80, Score: 0.00312\n",
      "Feature: 81, Score: 0.00632\n",
      "Feature: 82, Score: 0.00636\n",
      "Feature: 83, Score: 0.01028\n",
      "Feature: 84, Score: 0.01027\n",
      "Feature: 85, Score: 0.01663\n",
      "Feature: 86, Score: 0.00101\n",
      "Feature: 87, Score: 0.00104\n"
     ]
    }
   ],
   "source": [
    "importance = final_model.feature_importances_\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- We were able to create a model that has a score of 64% in predicting the type of a crash from the top 4 types.\n",
    "\n",
    "- This model uses 23 input factors to predict the type of crash.\n",
    "\n",
    "- The most important factors in finding the type of a crash are sex and age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Continue to iterate upon model:\n",
    "- This model was only trained on 21% of total crashes, and only predicts the top 4 crash types. Including other types of crash types will allow the model to predict additional crash types.\n",
    "- Improve performance by running additional gridsearches and checking other model types to see if they have better results.\n",
    "\n",
    "Look more into ‘Unable to determine’ cause:\n",
    "- It's likely that many crashes get lumped into this cause, when there is a true cause at hand. Can we discover the true cause based on model?\n",
    "- The number one crash type, containing more than half of all crashes, has to be thrown out due to being uninterpretable. Is there a better way of interpreting this cause?\n",
    "\n",
    "Deploy model:\n",
    "- Answer 23 simple questions to find cause of crash. May be useful to law enforcement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
